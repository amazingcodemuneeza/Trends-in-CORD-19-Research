{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30d86738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, GPTNeoForCausalLM, Trainer, TrainingArguments\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "370ed8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mubashir\\AppData\\Local\\Temp\\ipykernel_5252\\3379715936.py:1: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfb87590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Skipped 0 missing files.\n",
      "\n",
      "‚úÖ Total JSON files successfully read: 100\n"
     ]
    }
   ],
   "source": [
    "def read_json_files(paths):\n",
    "    \"\"\"Reads up to 100 valid JSON files from a list of paths, skipping non-existent or invalid ones.\"\"\"\n",
    "    json_data = []\n",
    "    skipped_files = []\n",
    "\n",
    "    for path in paths:\n",
    "        path = str(path).strip()\n",
    "        if not path or path.lower() == \"nan\" or not path.endswith(\".json\"):\n",
    "            continue\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    json_data.append(json.load(f))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ö†Ô∏è JSON decode error in file {path}: {e}\")\n",
    "        else:\n",
    "            skipped_files.append(path)\n",
    "\n",
    "        if len(json_data) >= 100:\n",
    "            break  # stop after successfully reading 100 JSON files\n",
    "\n",
    "    print(f\"üîç Skipped {len(skipped_files)} missing files.\")\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# Combine both JSON path columns and split all entries by ;\n",
    "df[\"all_json_files\"] = df[[\"pdf_json_files\", \"pmc_json_files\"]].astype(str).agg(\";\".join, axis=1)\n",
    "\n",
    "# Clean and extract individual paths\n",
    "all_paths = (\n",
    "    df[\"all_json_files\"]\n",
    "    .str.split(\";\")\n",
    "    .explode()\n",
    "    .dropna()\n",
    "    .map(str.strip)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Limit to first 100 paths\n",
    "limited_paths = all_paths[:200]  # Read a bit more to account for skipped ones\n",
    "\n",
    "# Read valid files only (max 100 successful reads)\n",
    "json_contents = read_json_files(limited_paths)\n",
    "\n",
    "print(f\"\\n‚úÖ Total JSON files successfully read: {len(json_contents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ded38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Helper function: Extract text from JSON\n",
    "# ---------------------------\n",
    "def extract_text(json_obj):\n",
    "    \"\"\"\n",
    "    Extracts and concatenates text from the 'abstract' and 'body_text' fields.\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "    # Process abstract (it can be a list or a string)\n",
    "    if \"abstract\" in json_obj:\n",
    "        if isinstance(json_obj[\"abstract\"], list):\n",
    "            for item in json_obj[\"abstract\"]:\n",
    "                if isinstance(item, str):\n",
    "                    text_parts.append(item)\n",
    "                elif isinstance(item, dict):\n",
    "                    text_parts.append(item.get(\"text\", \"\"))\n",
    "        elif isinstance(json_obj[\"abstract\"], str):\n",
    "            text_parts.append(json_obj[\"abstract\"])\n",
    "    \n",
    "    # Process body_text (usually a list of sections)\n",
    "    if \"body_text\" in json_obj:\n",
    "        if isinstance(json_obj[\"body_text\"], list):\n",
    "            for item in json_obj[\"body_text\"]:\n",
    "                if isinstance(item, dict):\n",
    "                    text_parts.append(item.get(\"text\", \"\"))\n",
    "                elif isinstance(item, str):\n",
    "                    text_parts.append(item)\n",
    "        elif isinstance(json_obj[\"body_text\"], str):\n",
    "            text_parts.append(json_obj[\"body_text\"])\n",
    "    \n",
    "    return \" \".join(text_parts).strip()\n",
    "\n",
    "# ---------------------------\n",
    "# Build DataFrame from JSON files\n",
    "# ---------------------------\n",
    "records = []\n",
    "for json_obj in json_contents:\n",
    "    paper_id = json_obj.get(\"paper_id\", \"Unknown\")\n",
    "    metadata = json_obj.get(\"metadata\", {})\n",
    "    title = metadata.get(\"title\", \"No Title\")\n",
    "    \n",
    "    # Extract authors as a comma-separated string\n",
    "    authors_list = metadata.get(\"authors\", [])\n",
    "    if isinstance(authors_list, list) and authors_list:\n",
    "        authors = []\n",
    "        for a in authors_list:\n",
    "            if isinstance(a, dict):\n",
    "                # Use \"name\" key if available, otherwise combine first and last names\n",
    "                if \"name\" in a:\n",
    "                    authors.append(a[\"name\"])\n",
    "                else:\n",
    "                    fname = a.get(\"first\", \"\")\n",
    "                    lname = a.get(\"last\", \"\")\n",
    "                    authors.append((fname + \" \" + lname).strip())\n",
    "            elif isinstance(a, str):\n",
    "                authors.append(a)\n",
    "        authors = \", \".join(authors)\n",
    "    else:\n",
    "        authors = \"Unknown\"\n",
    "    \n",
    "    # Extract combined text and compute statistics\n",
    "    text = extract_text(json_obj)\n",
    "    word_count = len(re.findall(r'\\w+', text))\n",
    "    char_count = len(text)\n",
    "    \n",
    "    # Extract sections from body_text (if available)\n",
    "    sections = []\n",
    "    if \"body_text\" in json_obj and isinstance(json_obj[\"body_text\"], list):\n",
    "        for item in json_obj[\"body_text\"]:\n",
    "            if isinstance(item, dict):\n",
    "                sec = item.get(\"section\", \"\").strip()\n",
    "                if sec:\n",
    "                    sections.append(sec)\n",
    "    sections_str = \", \".join(set(sections)) if sections else \"N/A\"\n",
    "    \n",
    "    records.append({\n",
    "        \"paper_id\": paper_id,\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"word_count\": word_count,\n",
    "        \"char_count\": char_count,\n",
    "        \"sections\": sections_str,\n",
    "        \"text\": text  # complete extracted text for further analysis\n",
    "    })\n",
    "\n",
    "eda_df = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd6c167f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "361f23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset from your DataFrame\n",
    "texts = eda_df[\"text\"].tolist()\n",
    "data = {\"text\": texts}\n",
    "dataset = Dataset.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e13f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Add pad token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00207b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 25.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    encoding = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()  # This is the key fix\n",
    "    return encoding\n",
    "# Tokenize and format dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c910cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and move it to GPU\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ece7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments with GPU support\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision if GPU supports it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e150e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer initialization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7ad0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 6/50 [00:00<00:05,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5528, 'grad_norm': 12.996912002563477, 'learning_rate': 4.7e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [00:01<00:05,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6893, 'grad_norm': 14.428654670715332, 'learning_rate': 4.2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:04<00:08,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5464, 'grad_norm': 13.377995491027832, 'learning_rate': 3.7e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:04<00:04,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5801, 'grad_norm': 13.512916564941406, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:07<00:05,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.804, 'grad_norm': 13.241741180419922, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:08<00:03,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5694, 'grad_norm': 12.353569984436035, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:11<00:03,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4782, 'grad_norm': 13.036826133728027, 'learning_rate': 1.8e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:11<00:01,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4286, 'grad_norm': 14.68310546875, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:14<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3845, 'grad_norm': 14.913464546203613, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:15<00:00,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4968, 'grad_norm': 14.447256088256836, 'learning_rate': 3e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:17<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17.2184, 'train_samples_per_second': 5.808, 'train_steps_per_second': 2.904, 'train_loss': 3.553013801574707, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=3.553013801574707, metrics={'train_runtime': 17.2184, 'train_samples_per_second': 5.808, 'train_steps_per_second': 2.904, 'total_flos': 6532300800000.0, 'train_loss': 3.553013801574707, 'epoch': 1.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50fdb915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetuned\\\\tokenizer_config.json',\n",
       " './gpt2-finetuned\\\\special_tokens_map.json',\n",
       " './gpt2-finetuned\\\\vocab.json',\n",
       " './gpt2-finetuned\\\\merges.txt',\n",
       " './gpt2-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4feb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, model, tokenizer, max_length=100):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(device)\n",
    "    output_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5517d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Answer: What does the paper say about COVID-19 in Russia?\n",
      "\n",
      "The paper says that the Russian government has been working on a plan to develop a new type of nuclear weapon, the \"Chernobyl-type\" nuclear weapon. The plan is to develop a new type of nuclear weapon, the \"Chernobyl-type\" nuclear weapon, which is a type of nuclear weapon that can be used to destroy the entire Soviet Union. The plan is to develop a new type of nuclear weapon\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"What does the paper say about COVID-19 in Russia?\"\n",
    "print(\"Query Answer:\", answer_query(query, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af562cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a458c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, model, tokenizer, max_length=100):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d865d331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Query: What are the symptoms of COVID-19?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: What are the symptoms of COVID-19?\n",
      "\n",
      "The symptoms of COVID-19 are similar to those of other respiratory infections, such as pneumonia, bronchiolitis, and bronchiolitis. The symptoms of COVID-19 are similar to those of other respiratory infections, such as pneumonia, bronchiolitis, and bronchiolitis. The symptoms of COVID-19 are similar to those of other respiratory infections, such as pneumonia, bronchiolitis, and\n",
      "\n",
      "üß† Query: What treatments were proposed for the coronavirus?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: What treatments were proposed for the coronavirus?\n",
      "\n",
      "The first was the coronavirus coronavirus (CVD). The coronavirus was first described in 1885 by the late Dr. William H. H. Haldane, Jr., who described it as \"a disease of the heart, which is the most common cause of death in the United States.\" The coronavirus was first described in 1885 by Dr. William H. Haldane, Jr., who described\n",
      "\n",
      "üß† Query: How does the virus spread between individuals?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: How does the virus spread between individuals?\n",
      "\n",
      "The virus is a viral RNA virus that is found in the blood of infected individuals. It is a viral RNA virus that is found in the blood of infected individuals. It is a viral RNA virus that is found in the blood of infected individuals. It is a viral RNA virus that is found in the blood of infected individuals. It is a viral RNA virus that is found in the blood of infected individuals. It is a viral RNA virus that is found\n",
      "\n",
      "üß† Query: What are the effects of lockdowns on mental health?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: What are the effects of lockdowns on mental health?\n",
      "\n",
      "The most common cause of lockdowns is a lack of access to mental health care. In the United States, approximately one in four people with mental illness is hospitalized for a mental health condition. In addition, approximately one in four people with mental illness is hospitalized for a mental health condition. In addition, approximately one in four people with mental illness is hospitalized for a mental health condition. In addition, approximately one in four people with mental illness\n",
      "\n",
      "üß† Query: Describe the impact of COVID-19 on the global economy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: Describe the impact of COVID-19 on the global economy.\n",
      "\n",
      "The impact of COVID-19 on the global economy is well known. It has been shown that the effects of COVID-19 on the global economy are well known. It has been shown that the effects of COVID-19 on the global economy is well known. It has been shown that the effects of COVID-19 on the global economy are well known. It has been shown that the effects of CO\n",
      "\n",
      "üß† Query: What role do vaccines play in controlling the pandemic?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: What role do vaccines play in controlling the pandemic?\n",
      "\n",
      "The role of vaccines in controlling the pandemic is well established. In the past, the role of vaccines has been largely ignored. In the present, however, the role of vaccines has been recognized. In the past, the role of vaccines has been largely ignored. In the present, however, the role of vaccines has been recognized. In the present, the role of vaccines has been recognized. In the present, the role of vaccines\n",
      "\n",
      "üß† Query: Explain the transmission mechanism of SARS-CoV-2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: Explain the transmission mechanism of SARS-CoV-2.\n",
      "\n",
      "SARS-CoV-2 is a viral RNA virus that is transmitted through the respiratory tract to the respiratory tract through the respiratory tract. It is a viral RNA virus that is transmitted through the respiratory tract to the respiratory tract through the respiratory tract. It is a viral RNA virus that is transmitted through the respiratory tract to the respiratory tract through the respiratory tract. It is a viral RNA virus that is transmitted through the respiratory\n",
      "\n",
      "üß† Query: How did COVID-19 affect healthcare systems in India?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: How did COVID-19 affect healthcare systems in India?\n",
      "\n",
      "The first step in understanding the effects of COVID-19 on healthcare systems is to understand the mechanisms by which it affects healthcare systems. The first step is to understand the mechanisms by which it affects healthcare systems. The first step in understanding the mechanisms by which it affects healthcare systems is to understand the mechanisms by which it affects healthcare systems.\n",
      "\n",
      "The first step in understanding the mechanisms by which it affects healthcare systems is to understand the\n",
      "\n",
      "üß† Query: Was there any research on mask effectiveness?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Answer: Was there any research on mask effectiveness?\n",
      "\n",
      "No, there was no research on mask effectiveness. The only study that we have done is to compare the effectiveness of different masks with the effectiveness of different masks. We have done this in a number of studies. We have also done this in a number of studies. We have also done this in a number of studies. We have also done this in a number of studies. We have also done this in a number of studies. We have also done\n",
      "\n",
      "üß† Query: What mutations have been identified in COVID-19 variants?\n",
      "üìÑ Answer: What mutations have been identified in COVID-19 variants? The most common mutations are:\n",
      "\n",
      "1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What are the symptoms of COVID-19?\",\n",
    "    \"What treatments were proposed for the coronavirus?\",\n",
    "    \"How does the virus spread between individuals?\",\n",
    "    \"What are the effects of lockdowns on mental health?\",\n",
    "    \"Describe the impact of COVID-19 on the global economy.\",\n",
    "    \"What role do vaccines play in controlling the pandemic?\",\n",
    "    \"Explain the transmission mechanism of SARS-CoV-2.\",\n",
    "    \"How did COVID-19 affect healthcare systems in India?\",\n",
    "    \"Was there any research on mask effectiveness?\",\n",
    "    \"What mutations have been identified in COVID-19 variants?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nüß† Query: {q}\")\n",
    "    print(\"üìÑ Answer:\", answer_query(q, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c589d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 397.21 examples/s]\n",
      " 12%|‚ñà‚ñè        | 6/50 [00:00<00:04, 10.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7993, 'grad_norm': 8.838666915893555, 'learning_rate': 4.7e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [00:00<00:03, 10.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9894, 'grad_norm': 10.019515037536621, 'learning_rate': 4.2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:02<00:06,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8827, 'grad_norm': 9.032402992248535, 'learning_rate': 3.7e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:03<00:04,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9407, 'grad_norm': 10.409589767456055, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:05<00:05,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0982, 'grad_norm': 9.586071968078613, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:05<00:02,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9465, 'grad_norm': 9.354372024536133, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:07<00:03,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7724, 'grad_norm': 9.076519966125488, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:07<00:01,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7067, 'grad_norm': 9.850354194641113, 'learning_rate': 1.2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:09<00:00,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7834, 'grad_norm': 10.898308753967285, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:10<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8578, 'grad_norm': 9.900525093078613, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:11<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.7616, 'train_samples_per_second': 8.502, 'train_steps_per_second': 4.251, 'train_loss': 3.8776962661743166, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilGPT-2 Answer: What does the paper say about COVID-19 in Russia?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer using AutoTokenizer and specify the DistilGPT-2 model name\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # DistilGPT-2 uses the eos token as its pad token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize inputs and set labels for language modeling\n",
    "    encoding = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()  # Required for Trainer to compute loss\n",
    "    return encoding\n",
    "\n",
    "# Tokenize and format the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Load the model and move it to GPU if available\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model = model.to(device)\n",
    "\n",
    "# Define training arguments with GPU (and mixed precision if available)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilgpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,                # Adjust as needed\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=torch.cuda.is_available(),    # Enable FP16 mixed precision on GPUs\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./distilgpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./distilgpt2-finetuned\")\n",
    "\n",
    "# Define a simple text generation function for querying the model\n",
    "def answer_query(query, model, tokenizer, max_length=100):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example query usage:\n",
    "query = \"What does the paper say about COVID-19 in Russia?\"\n",
    "print(\"DistilGPT-2 Answer:\", answer_query(query, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6b7c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 365.30 examples/s]\n",
      " 12%|‚ñà‚ñè        | 6/50 [00:01<00:06,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9467, 'grad_norm': 7.538763046264648, 'learning_rate': 4.5e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [00:01<00:06,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0882, 'grad_norm': 9.881985664367676, 'learning_rate': 4e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:04<00:09,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9975, 'grad_norm': 9.502613067626953, 'learning_rate': 3.5e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:05<00:05,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9798, 'grad_norm': 7.901386260986328, 'learning_rate': 3e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:08<00:06,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1388, 'grad_norm': 8.603156089782715, 'learning_rate': 2.5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:08<00:03,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9294, 'grad_norm': 7.441476821899414, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:11<00:03,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9809, 'grad_norm': 7.939748764038086, 'learning_rate': 1.5e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:12<00:01,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9915, 'grad_norm': 8.829133033752441, 'learning_rate': 1e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:15<00:01,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7363, 'grad_norm': 8.229199409484863, 'learning_rate': 5e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:16<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8677, 'grad_norm': 8.186396598815918, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:18<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 18.1667, 'train_samples_per_second': 5.505, 'train_steps_per_second': 2.752, 'train_loss': 2.9656890487670897, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-Neo Answer: Summarize the COVID-19 research findings from the paper. The authors have reviewed the paper and have made a number of comments. The authors have also reviewed the paper and have made comments on the manuscript. The authors have reviewed the paper and have made comments on the manuscript. The authors have reviewed the paper and have made comments on the manuscript. The authors have reviewed the paper and have made comments on the manuscript. The authors have reviewed the paper and have made comments on the manuscript. The\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model for GPT-Neo using AutoTokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-Neo uses the eos token as pad\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    encoding = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n",
    "    return encoding\n",
    "\n",
    "# Tokenize and format dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Load model and move to device\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model = model.to(device)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gptneo-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU available\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./gptneo-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gptneo-finetuned\")\n",
    "\n",
    "# Define a simple text generation function\n",
    "def answer_query(query, model, tokenizer, max_length=100):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage:\n",
    "query = \"Summarize the COVID-19 research findings from the paper.\"\n",
    "print(\"GPT-Neo Answer:\", answer_query(query, model, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
